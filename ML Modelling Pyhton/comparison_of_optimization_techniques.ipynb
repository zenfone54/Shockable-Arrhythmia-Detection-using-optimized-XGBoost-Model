{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DBpylAqOSqz8",
        "HGmYNsUzdKaP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-hUBmXKRNJ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import xgboost as xgb\n",
        "\n",
        "df = pd.read_csv(\"D:\\Capstone\\Databases\\ML CSV\\CUDB_VFDB_combined.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfxVySKRRZ-E",
        "outputId": "6ab21559-f478-458e-850f-2632d395e3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12775, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the data into two classes\n",
        "class_0 = df[df[\"rhythm\"] == 0]\n",
        "class_1 = df[df[\"rhythm\"] == 1]\n",
        "\n",
        "# Undersample class 0 to match the number of samples in class 1\n",
        "class_0_undersampled = class_0.sample(n=len(class_1), random_state=42)\n",
        "\n",
        "# Concatenate the undersampled class 0 with class 1\n",
        "balanced_df = pd.concat([class_0_undersampled, class_1])\n",
        "\n",
        "# Shuffle the concatenated DataFrame to mix the rows\n",
        "#balanced_df = balanced_df.sample(frac=1, random_state=100)"
      ],
      "metadata": {
        "id": "57pZ5VzYRb-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(balanced_df['rhythm'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1MpSOJLRda9",
        "outputId": "33331388-4ad3-446e-c5ef-131b6df6ecb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rhythm\n",
            "0    2399\n",
            "1    2399\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X = balanced_df.drop(columns=['std_dev','rhythm','STE'])\n",
        "X = balanced_df.iloc[:, :-1]\n",
        "y = balanced_df.iloc[:, -1]"
      ],
      "metadata": {
        "id": "vMUdPIMVRnqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 100)"
      ],
      "metadata": {
        "id": "lS485IB-RfMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "'''\n",
        "model = xgb.XGBClassifier(colsample_bytree = 0.816432335230161,\n",
        "gamma = 0,\n",
        "learning_rate = 0.060309950161179966,\n",
        "max_depth = 10,\n",
        "n_estimators = 500,\n",
        "reg_alpha = 0,\n",
        "reg_lambda = 7,\n",
        "subsample = 0.9946951308154663)\n",
        "'''\n",
        "'''\n",
        "model = xgb.XGBClassifier(colsample_bytree = 1,\n",
        "gamma = 0,\n",
        "learning_rate = 0.17901706233426795,\n",
        "max_depth = 10,\n",
        "n_estimators = 366,\n",
        "reg_alpha = 0,\n",
        "reg_lambda = 5,\n",
        "subsample = 1)\n",
        "'''\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "y_predict = model.predict(X_test)\n",
        "y_train_predict = model.predict(X_train)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Train accuracy',accuracy_score(y_train, y_train_predict))\n",
        "print('Test accuracy',accuracy_score(y_test,y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swI3okQDRjhV",
        "outputId": "027f4417-c452-4a35-c436-574e0a09a38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy 1.0\n",
            "Test accuracy 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Fit the model\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_predict_train = model.predict(X_train)\n",
        "y_predict_test = model.predict(X_test)\n",
        "\n",
        "# Metrics calculation\n",
        "accuracy_train = accuracy_score(y_train, y_predict_train)\n",
        "accuracy_test = accuracy_score(y_test, y_predict_test)\n",
        "sensitivity = recall_score(y_test, y_predict_test)\n",
        "specificity = recall_score(y_test, y_predict_test, pos_label=0)\n",
        "precision = precision_score(y_test, y_predict_test)\n",
        "npv = precision_score(y_test, y_predict_test, pos_label=0)\n",
        "auc_roc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Convert to percentages\n",
        "accuracy_train *= 100\n",
        "accuracy_test *= 100\n",
        "sensitivity *= 100\n",
        "specificity *= 100\n",
        "precision *= 100\n",
        "npv *= 100\n",
        "auc_roc *= 100\n",
        "\n",
        "# Print results\n",
        "print(f'Train Accuracy: {accuracy_train:.2f}%')\n",
        "print(f'Test Accuracy: {accuracy_test:.2f}%')\n",
        "print(f'Sensitivity: {sensitivity:.2f}%')\n",
        "print(f'Specificity: {specificity:.2f}%')\n",
        "print(f'Precision: {precision:.2f}%')\n",
        "print(f'NPV: {npv:.2f}%')\n",
        "print(f'AUC-ROC: {auc_roc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUgpurpVELJl",
        "outputId": "171fa18f-77ee-4a94-afa4-31f1ca5b5a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 100.00%\n",
            "Test Accuracy: 93.33%\n",
            "Sensitivity: 93.53%\n",
            "Specificity: 93.14%\n",
            "Precision: 93.14%\n",
            "NPV: 93.53%\n",
            "AUC-ROC: 98.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "# Create a heatmap visualization using Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eGbCgzxbQgdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Optimization (skopt)**"
      ],
      "metadata": {
        "id": "WF1LSnh4S8Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_space = {\n",
        "    'n_estimators': (50, 500),\n",
        "    'max_depth': (3, 10),\n",
        "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
        "    'subsample': (0.5, 1.0),\n",
        "    'colsample_bytree': (0.5, 1.0),\n",
        "    'gamma': (0, 5),\n",
        "    'reg_alpha': (0, 10),\n",
        "    'reg_lambda': (0, 10),\n",
        "}\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = XGBClassifier()\n",
        "\n",
        "# Initialize BayesSearchCV with the classifier, parameter space, and number of iterations\n",
        "opt = BayesSearchCV(\n",
        "    clf,\n",
        "    param_space,\n",
        "    n_iter=50,  # Number of iterations\n",
        "    cv=5,       # Cross-validation folds\n",
        "    n_jobs=-1,  # Number of CPU cores to use (-1 uses all available)\n",
        "    scoring='accuracy',  # Scoring metric\n",
        "    random_state=42\n",
        ")\n",
        "np.int = int\n",
        "\n",
        "# Fit the BayesSearchCV to the data\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by Bayesian optimization:\")\n",
        "print(opt.best_params_)\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = opt.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the best model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_l5RZtGS6sI",
        "outputId": "60a05e0f-c956-4e0e-826b-d2f4c31cbf86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by Bayesian optimization:\n",
            "OrderedDict([('colsample_bytree', 1.0), ('gamma', 0), ('learning_rate', 0.16887504568911432), ('max_depth', 10), ('n_estimators', 500), ('reg_alpha', 0), ('reg_lambda', 10), ('subsample', 1.0)])\n",
            "Accuracy of the best model: 0.94375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score\n",
        "\n",
        "# Calculate confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn) * 100\n",
        "\n",
        "# Calculate sensitivity (recall)\n",
        "sensitivity = tp / (tp + fn) * 100\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = tn / (tn + fp) * 100\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred) * 100\n",
        "\n",
        "# Calculate NPV\n",
        "npv = tn / (tn + fn) * 100\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "auc_roc = roc_auc_score(y_test, opt.predict_proba(X_test)[:, 1]) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Sensitivity: {sensitivity:.2f}%\")\n",
        "print(f\"Specificity: {specificity:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"NPV: {npv:.2f}%\")\n",
        "print(f\"AUC-ROC: {auc_roc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SePfRfZT8lu",
        "outputId": "de47e7ea-7024-49ac-c9ab-80ba78b0deeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.38%\n",
            "Sensitivity: 95.35%\n",
            "Specificity: 93.43%\n",
            "Precision: 93.37%\n",
            "NPV: 95.39%\n",
            "AUC-ROC: 98.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Optimization (Hyperopt)**"
      ],
      "metadata": {
        "id": "cyZGa6CxO3lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from hyperopt import hp, tpe, fmin, Trials\n",
        "\n",
        "\n",
        "\n",
        "# Define the objective function to minimize (negative accuracy)\n",
        "def objective(params):\n",
        "    # Convert float hyperparameters to integers\n",
        "    params[\"n_estimators\"] = int(params[\"n_estimators\"])\n",
        "    params[\"max_depth\"] = int(params[\"max_depth\"])\n",
        "\n",
        "    # Initialize XGBoost classifier with given hyperparameters\n",
        "    clf = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Return negative accuracy (to minimize)\n",
        "    return -accuracy\n",
        "\n",
        "# Define the search space\n",
        "space = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 500, 1),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'gamma': hp.uniform('gamma', 0, 5),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0, 10)\n",
        "}\n",
        "\n",
        "# Initialize trials object to keep track of optimization history\n",
        "trials = Trials()\n",
        "\n",
        "# Run Bayesian optimization\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "# Convert best parameters to integer where needed\n",
        "best[\"n_estimators\"] = int(best[\"n_estimators\"])\n",
        "best[\"max_depth\"] = int(best[\"max_depth\"])\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best)\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "best_clf = xgb.XGBClassifier(**best)\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = best_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYnEmxD7O7HT",
        "outputId": "dd6fcd8c-b6a0-483b-e722-1bcf309da490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████████████| 50/50 [00:08<00:00,  5.94trial/s, best loss: -0.9395833333333333]\n",
            "Best Hyperparameters: {'colsample_bytree': 0.9180735836279931, 'gamma': 0.040176865265566145, 'learning_rate': 0.09204233967922984, 'max_depth': 9, 'n_estimators': 487, 'reg_alpha': 4.675886120647191, 'reg_lambda': 3.475975666606203, 'subsample': 0.7699154692375176}\n",
            "Accuracy: 0.9395833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Calculate confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "\n",
        "# Calculate sensitivity (recall)\n",
        "sensitivity = (tp / (tp + fn)) * 100\n",
        "\n",
        "# Calculate specificity\n",
        "specificity = (tn / (tn + fp)) * 100\n",
        "\n",
        "# Calculate precision\n",
        "precision = (tp / (tp + fp)) * 100\n",
        "\n",
        "# Calculate NPV\n",
        "npv = (tn / (tn + fn)) * 100\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "auc_roc = roc_auc_score(y_test, y_pred) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Sensitivity: {sensitivity:.2f}%\")\n",
        "print(f\"Specificity: {specificity:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"NPV: {npv:.2f}%\")\n",
        "print(f\"AUC-ROC: {auc_roc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUJ-Xrs1REcj",
        "outputId": "ab9f3d0b-6adb-4209-c26a-09fa0edd469a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 93.96%\n",
            "Sensitivity: 94.71%\n",
            "Specificity: 93.22%\n",
            "Precision: 93.14%\n",
            "NPV: 94.78%\n",
            "AUC-ROC: 93.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomizedSearch**"
      ],
      "metadata": {
        "id": "YWCGYCbsmrri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "# Define the hyperparameter search space (same as for Bayesian optimization)\n",
        "param_space = {\n",
        "    'n_estimators': range(50, 501),\n",
        "    'max_depth': range(3, 11),\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.4, 1.0],\n",
        "    'subsample': [0.5, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.5, 0.8,1.0],\n",
        "    'gamma': [0, 1, 2, 3, 4, 5],\n",
        "    'reg_alpha': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'reg_lambda': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "}\n",
        "\n",
        "# Define the RandomizedSearchCV object with 50 iterations\n",
        "random_search = RandomizedSearchCV(\n",
        "    clf,\n",
        "    param_distributions=param_space,\n",
        "    n_iter=150,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecf9hNjVnSgs",
        "outputId": "9e207418-33d3-4364-e4d9-bd27d8b8b51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 416, 'max_depth': 10, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Train the XGBoost model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBClassifier(**best_params)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "sensitivity = recall_score(y_test, y_pred)  # Sensitivity is also known as recall\n",
        "specificity = accuracy_score(y_test, y_pred, normalize=True)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "npv = tn / (tn + fn)  # Negative Predictive Value\n",
        "auc_roc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "# Print the values of evaluation metrics in percentages up to two decimal places\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
        "print(\"Sensitivity (Recall): {:.2f}%\".format(sensitivity * 100))\n",
        "print(\"Specificity: {:.2f}%\".format(specificity * 100))\n",
        "print(\"NPV (Negative Predictive Value): {:.2f}%\".format(npv * 100))\n",
        "print(\"AUC-ROC: {:.2f}%\".format(auc_roc * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_ZY5JQxxetX",
        "outputId": "b99a12d8-5da8-4c9e-8c66-26bb1dabcdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 93.65%\n",
            "Precision: 92.92%\n",
            "Sensitivity (Recall): 94.29%\n",
            "Specificity: 93.65%\n",
            "NPV (Negative Predictive Value): 94.38%\n",
            "AUC-ROC: 93.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PSO**"
      ],
      "metadata": {
        "id": "DBpylAqOSqz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PSO\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pyswarm import pso\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "# Define the function to optimize (minimize)\n",
        "def objective_function(hyperparameters):\n",
        "    # Convert hyperparameters to dictionary\n",
        "    param_dict = {\n",
        "        'n_estimators': int(hyperparameters[0]),\n",
        "        'max_depth': int(hyperparameters[1]),\n",
        "        'learning_rate': hyperparameters[2],\n",
        "        'subsample': hyperparameters[3],\n",
        "        'colsample_bytree': hyperparameters[4],\n",
        "        'gamma': hyperparameters[5],\n",
        "        'reg_alpha': hyperparameters[6],\n",
        "        'reg_lambda': hyperparameters[7]\n",
        "    }\n",
        "\n",
        "    # Set hyperparameters for the classifier\n",
        "    clf.set_params(**param_dict)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # PSO minimizes the objective function, so we return negative accuracy\n",
        "    return -accuracy\n",
        "\n",
        "# Define the bounds for each hyperparameter\n",
        "lb = [50, 3, 0.01, 0.5, 0.5, 0, 0, 0]  # Lower bounds\n",
        "ub = [500, 10, 1.0, 1.0, 1.0, 5, 10, 10]  # Upper bounds\n",
        "\n",
        "# Run PSO to optimize hyperparameters\n",
        "best_hyperparameters, _ = pso(objective_function, lb, ub, swarmsize=10, maxiter=50)\n",
        "\n",
        "# Convert hyperparameters to dictionary\n",
        "best_param_dict = {\n",
        "    'n_estimators': int(best_hyperparameters[0]),\n",
        "    'max_depth': int(best_hyperparameters[1]),\n",
        "    'learning_rate': best_hyperparameters[2],\n",
        "    'subsample': best_hyperparameters[3],\n",
        "    'colsample_bytree': best_hyperparameters[4],\n",
        "    'gamma': best_hyperparameters[5],\n",
        "    'reg_alpha': best_hyperparameters[6],\n",
        "    'reg_lambda': best_hyperparameters[7]\n",
        "}\n",
        "\n",
        "# Set the best hyperparameters for the classifier\n",
        "clf.set_params(**best_param_dict)\n",
        "\n",
        "# Train the classifier with the best hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set with the best classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy with the best classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Accuracy:\", accuracy)\n",
        "print(\"Best Hyperparameters:\", best_param_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfn60iI4YjkC",
        "outputId": "3ba39be5-ca38-447f-ed78-2a1350006303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 50\n",
            "Best Accuracy: 0.9458333333333333\n",
            "Best Hyperparameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.2619945862958502, 'subsample': 0.6218596122214207, 'colsample_bytree': 0.9870653101081508, 'gamma': 0.041294248394420405, 'reg_alpha': 1.1706355025369426, 'reg_lambda': 8.339934590841537}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "\n",
        "# Calculate sensitivity (true positive rate)\n",
        "sensitivity = recall_score(y_test, y_pred) * 100\n",
        "\n",
        "# Calculate specificity (true negative rate)\n",
        "specificity = (tn / (tn + fp)) * 100\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred) * 100\n",
        "\n",
        "# Calculate NPV (Negative Predictive Value)\n",
        "npv = (tn / (tn + fn)) * 100\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "auc_roc = roc_auc_score(y_test, y_pred_proba) * 100\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
        "print(\"Sensitivity (Recall): {:.2f}%\".format(sensitivity))\n",
        "print(\"Specificity: {:.2f}%\".format(specificity))\n",
        "print(\"Precision: {:.2f}%\".format(precision))\n",
        "print(\"NPV (Negative Predictive Value): {:.2f}%\".format(npv))\n",
        "print(\"AUC-ROC: {:.2f}%\".format(auc_roc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7bNvnePZ07Y",
        "outputId": "c14a42f0-c11d-4326-c78a-4b26558cd993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.17%\n",
            "Sensitivity (Recall): 94.93%\n",
            "Specificity: 93.43%\n",
            "Precision: 93.35%\n",
            "NPV (Negative Predictive Value): 94.99%\n",
            "AUC-ROC: 97.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simualted annealing**"
      ],
      "metadata": {
        "id": "HGmYNsUzdKaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install simanneal\n"
      ],
      "metadata": {
        "id": "4MzyyD1-dMhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.optimize import dual_annealing\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "# Load your data and split into train and test sets\n",
        "# Replace X_train, X_test, y_train, and y_test with your actual data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the objective function\n",
        "def objective_function(hyperparameters):\n",
        "    param_dict = {\n",
        "        'n_estimators': int(hyperparameters[0]),\n",
        "        'max_depth': int(hyperparameters[1]),\n",
        "        'learning_rate': hyperparameters[2],\n",
        "        'subsample': hyperparameters[3],\n",
        "        'colsample_bytree': hyperparameters[4],\n",
        "        'gamma': hyperparameters[5],\n",
        "        'reg_alpha': hyperparameters[6],\n",
        "        'reg_lambda': hyperparameters[7]\n",
        "    }\n",
        "\n",
        "    # Set hyperparameters for the classifier\n",
        "    clf.set_params(**param_dict)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Return negative accuracy since scipy minimizes the objective function\n",
        "    return -accuracy\n",
        "\n",
        "# Define the bounds for each hyperparameter\n",
        "bounds = [(50, 500),  # n_estimators\n",
        "          (3, 10),    # max_depth\n",
        "          (0.01, 1.0),  # learning_rate\n",
        "          (0.5, 1.0),  # subsample\n",
        "          (0.5, 1.0),  # colsample_bytree\n",
        "          (0, 5),      # gamma\n",
        "          (0, 10),     # reg_alpha\n",
        "          (0, 10)]     # reg_lambda\n",
        "\n",
        "# Run simulated annealing to optimize hyperparameters\n",
        "result = dual_annealing(objective_function, bounds, maxiter=50)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hyperparameters = result.x\n",
        "\n",
        "# Convert best hyperparameters to dictionary\n",
        "best_param_dict = {\n",
        "    'n_estimators': int(best_hyperparameters[0]),\n",
        "    'max_depth': int(best_hyperparameters[1]),\n",
        "    'learning_rate': best_hyperparameters[2],\n",
        "    'subsample': best_hyperparameters[3],\n",
        "    'colsample_bytree': best_hyperparameters[4],\n",
        "    'gamma': best_hyperparameters[5],\n",
        "    'reg_alpha': best_hyperparameters[6],\n",
        "    'reg_lambda': best_hyperparameters[7]\n",
        "}\n",
        "\n",
        "# Set the best hyperparameters for the classifier\n",
        "clf.set_params(**best_param_dict)\n",
        "\n",
        "# Train the classifier with the best hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set with the best classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy with the best classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Accuracy:\", accuracy)\n",
        "print(\"Best Hyperparameters:\", best_param_dict)\n"
      ],
      "metadata": {
        "id": "G0jmdOCygZIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd35d75-af6d-49f9-df74-1f96c92c0d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy: 0.9427083333333334\n",
            "Best Hyperparameters: {'n_estimators': 330, 'max_depth': 6, 'learning_rate': 0.22418002399966896, 'subsample': 0.6414961479604244, 'colsample_bytree': 0.9148824531584978, 'gamma': 0.13911383599042892, 'reg_alpha': 2.830417461693287, 'reg_lambda': 0.1601114198565483}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc, precision_score, recall_score\n",
        "\n",
        "# Define a function to calculate sensitivity, specificity, precision, and NPV\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn) * 100\n",
        "    specificity = tn / (tn + fp) * 100\n",
        "    precision = tp / (tp + fp) * 100\n",
        "    npv = tn / (tn + fn) * 100\n",
        "\n",
        "    return sensitivity, specificity, precision, npv\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calculate accuracy, sensitivity, specificity, precision, and NPV\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "sensitivity, specificity, precision, npv = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % accuracy)\n",
        "print(\"Sensitivity: %.2f%%\" % sensitivity)\n",
        "print(\"Specificity: %.2f%%\" % specificity)\n",
        "print(\"Precision: %.2f%%\" % precision)\n",
        "print(\"NPV: %.2f%%\" % npv)\n",
        "print(\"AUC-ROC: %.2f%%\" % (roc_auc * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE3mQwO5pZ9u",
        "outputId": "fd33998b-c972-4f2a-8e36-7e3aa4df7d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.27%\n",
            "Sensitivity: 95.14%\n",
            "Specificity: 93.43%\n",
            "Precision: 93.36%\n",
            "NPV: 95.19%\n",
            "AUC-ROC: 98.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Genetic Algorithm**"
      ],
      "metadata": {
        "id": "Ps9Av4nPqwki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geneticalgorithm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyZJXEeKtDq7",
        "outputId": "6f59c8a6-3094-430a-8d2b-fb5a9b3aa860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geneticalgorithm\n",
            "  Obtaining dependency information for geneticalgorithm from https://files.pythonhosted.org/packages/ac/d2/fb9061239eaeee5c0373844f27f43514f33201bc08aea54d65b437402966/geneticalgorithm-1.0.2-py3-none-any.whl.metadata\n",
            "  Downloading geneticalgorithm-1.0.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting func-timeout (from geneticalgorithm)\n",
            "  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n",
            "     ---------------------------------------- 0.0/44.3 kB ? eta -:--:--\n",
            "     --------------------------- ------------ 30.7/44.3 kB 1.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 44.3/44.3 kB 1.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: numpy in e:\\softwares\\anaconda3\\lib\\site-packages (from geneticalgorithm) (1.24.3)\n",
            "Downloading geneticalgorithm-1.0.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: func-timeout\n",
            "  Building wheel for func-timeout (setup.py): started\n",
            "  Building wheel for func-timeout (setup.py): finished with status 'done'\n",
            "  Created wheel for func-timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15110 sha256=41e1b4cfb96736e8f99ccf04066e76c6489ef997f48fb18cb0b6af2d903ba6c6\n",
            "  Stored in directory: c:\\users\\subham\\appdata\\local\\pip\\cache\\wheels\\07\\e6\\86\\f23164d12c3134966614102db8e7956ab359faf7ffd78703ce\n",
            "Successfully built func-timeout\n",
            "Installing collected packages: func-timeout, geneticalgorithm\n",
            "Successfully installed func-timeout-4.3.5 geneticalgorithm-1.0.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using deap lib\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "# Define the function to optimize (maximize)\n",
        "def objective_function(hyperparameters):\n",
        "    # Convert hyperparameters to dictionary\n",
        "    param_dict = {\n",
        "        'n_estimators': int(hyperparameters[0]),\n",
        "        'max_depth': int(hyperparameters[1]),\n",
        "        'learning_rate': hyperparameters[2],\n",
        "        'subsample': hyperparameters[3],\n",
        "        'colsample_bytree': hyperparameters[4],\n",
        "        'gamma': hyperparameters[5],\n",
        "        'reg_alpha': hyperparameters[6],\n",
        "        'reg_lambda': hyperparameters[7]\n",
        "    }\n",
        "\n",
        "    # Set hyperparameters for the classifier\n",
        "    clf.set_params(**param_dict)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Return negative accuracy to maximize\n",
        "    return -accuracy\n",
        "\n",
        "# Define the bounds for each hyperparameter\n",
        "varbound = np.array([[50, 500],  # n_estimators\n",
        "                     [3, 10],    # max_depth\n",
        "                     [0.01, 1.0],  # learning_rate\n",
        "                     [0.5, 1.0],  # subsample\n",
        "                     [0.5, 1.0],  # colsample_bytree\n",
        "                     [0, 5],     # gamma\n",
        "                     [0, 10],    # reg_alpha\n",
        "                     [0, 10]])   # reg_lambda\n",
        "\n",
        "# Define the creator for the fitness and individual\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Initialize toolbox\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Register attributes to the individual\n",
        "toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=8)\n",
        "\n",
        "# Register the individual and population\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Register the evaluation function\n",
        "toolbox.register(\"evaluate\", objective_function)\n",
        "\n",
        "# Register the crossover operator\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "\n",
        "# Register the mutation operator\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.1)\n",
        "\n",
        "# Register the selection operator\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Define main function to run genetic algorithm\n",
        "def main():\n",
        "    # Initialize population\n",
        "    population = toolbox.population(n=10)\n",
        "\n",
        "    # Define statistics to gather\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"max\", np.max)\n",
        "\n",
        "    # Define the algorithm\n",
        "    algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=50, stats=stats, verbose=True)\n",
        "\n",
        "    # Get the best individual\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "\n",
        "    return best_individual\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "best_individual = main()\n",
        "\n",
        "# Convert the best individual to hyperparameters\n",
        "best_hyperparameters = [int(val) if idx == 0 else val for idx, val in enumerate(best_individual)]\n",
        "\n",
        "# Convert hyperparameters to dictionary\n",
        "best_param_dict = {\n",
        "    'n_estimators': int(best_hyperparameters[0]),\n",
        "    'max_depth': int(best_hyperparameters[1]),\n",
        "    'learning_rate': best_hyperparameters[2],\n",
        "    'subsample': best_hyperparameters[3],\n",
        "    'colsample_bytree': best_hyperparameters[4],\n",
        "    'gamma': best_hyperparameters[5],\n",
        "    'reg_alpha': best_hyperparameters[6],\n",
        "    'reg_lambda': best_hyperparameters[7]\n",
        "}\n",
        "\n",
        "# Set the best hyperparameters for the classifier\n",
        "clf.set_params(**best_param_dict)\n",
        "\n",
        "# Train the classifier with the best hyperparameters\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set with the best classifier\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy with the best classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Accuracy:\", accuracy)\n",
        "print(\"Best Hyperparameters:\", best_param_dict)\n"
      ],
      "metadata": {
        "id": "ZXYKgF7Lvq_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geatpy\n"
      ],
      "metadata": {
        "id": "W6l-Tou9y4Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from geatpy import GeneticAlgorithm\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "# Define the function to optimize (maximize)\n",
        "def objective_function(solution, *args):\n",
        "    X_train, X_test, y_train, y_test = args\n",
        "\n",
        "    # Convert the solution to hyperparameters\n",
        "    n_estimators = int(solution[0])\n",
        "    max_depth = int(solution[1])\n",
        "    learning_rate = solution[2]\n",
        "    subsample = solution[3]\n",
        "    colsample_bytree = solution[4]\n",
        "    gamma = solution[5]\n",
        "    reg_alpha = solution[6]\n",
        "    reg_lambda = solution[7]\n",
        "\n",
        "    # Set hyperparameters for the classifier\n",
        "    clf.set_params(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
        "                   subsample=subsample, colsample_bytree=colsample_bytree,\n",
        "                   gamma=gamma, reg_alpha=reg_alpha, reg_lambda=reg_lambda)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the bounds for each hyperparameter\n",
        "lb = [50, 3, 0.01, 0.5, 0.5, 0, 0, 0]  # Lower bounds\n",
        "ub = [500, 10, 1.0, 1.0, 1.0, 5, 10, 10]  # Upper bounds\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
        "\n",
        "# Set the parameters for the genetic algorithm\n",
        "problem = 'R'  # Real-coded optimization problem\n",
        "max_iter = 50  # Maximum number of iterations\n",
        "\n",
        "# Create an instance of the genetic algorithm\n",
        "ga = GeneticAlgorithm(problem, None, lb, ub, NIND=50, MAXGEN=max_iter)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "best_solution, best_fitness = ga.run(objective_function, X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"Best Solution:\", best_solution)\n",
        "print(\"Best Fitness (Accuracy):\", best_fitness)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "67s8XUbNymou",
        "outputId": "a7690ca6-eb41-4b93-ff2f-51536f370f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'geatpy'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeatpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneticAlgorithm\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Define the XGBoost classifier\u001b[39;00m\n\u001b[0;32m      8\u001b[0m clf \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geatpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall Cython\n"
      ],
      "metadata": {
        "id": "nK2mDwU70NY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall geatpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKMoAKgtzyGT",
        "outputId": "e3c71e1a-b4b0-4d09-82fc-1795133119cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping geatpy as it is not installed.\n"
          ]
        }
      ]
    }
  ]
}